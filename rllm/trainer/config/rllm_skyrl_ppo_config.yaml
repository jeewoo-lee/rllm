# SkyRL Backend Configuration for rLLM
# This config is used when training agents with SkyRL backend
# SkyRL uses BasePPOExp which expects generator.* structure instead of actor_rollout_ref.*

hydra:
  searchpath:
    - pkg://skyrl_train.config  # SkyRL's config search path

defaults:
  - ppo_trainer  # Base SkyRL PPO trainer config
  - _self_

# Generator Configuration (SkyRL-specific)
generator:
  model:
    path: Qwen/Qwen3-4B-Instruct-2507  # Model path for inference
  sampling_params:
    temperature: 1.0
    top_p: 1.0
    max_tokens: 1024
  n_parallel_tasks: 128  # Number of parallel workflow tasks
  retry_limit: 3  # Retry limit for workflow execution
  workflow_args: null  # Optional workflow args (can be overridden via workflow_args parameter)

# Data Configuration
data:
  train_files: null
  val_files: null
  train_batch_size: 64
  val_batch_size: 32
  max_prompt_length: 2048
  max_response_length: 1024

# Algorithm Configuration
algorithm:
  adv_estimator: grpo  # REINFORCE, GRPO

# rLLM Configuration
rllm:
  workflow:
    use_workflow: True  # SkyRL backend requires workflow mode
    n_parallel_tasks: 128
    retry_limit: 3
    raise_on_error: True
  stepwise_advantage:
    enable: True
    mode: per_step  # [broadcast, per_step]
    normalize_by_steps: False
  compact_filtering:
    enable: False
    mask_max_prompt_length_exceeded: True
    mask_max_response_length_exceeded: True
    mask_max_turns_exceeded: False
    mask_timeout: True
    mask_unknown: False
    mask_error: True
  rejection_sample:
    enable: False
    multiplier: 1.0
  disable_thinking: False
  accumulate_reasoning: False
  mask_truncated_samples: False
  filter_token_mismatch: True

# Trainer Configuration
trainer:
  logger: ['console', 'wandb']
  project_name: 'solver-judge-workflow-skyrl'
  experiment_name: 'countdown-solver-judge-skyrl'
  val_before_train: True
  n_gpus_per_node: 8
  nnodes: 1
  save_freq: 1000
  test_freq: 10
  total_epochs: 100
  default_hdfs_dir: null
  log_episodes: false
  episode_log_dir: logs/${trainer.project_name}/${trainer.experiment_name}

# Ray Configuration
ray_init:
  _temp_dir: /tmp

